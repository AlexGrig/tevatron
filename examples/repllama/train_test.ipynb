{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/azureuser/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModel\n",
    "import dataclasses\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    ")\n",
    "\n",
    "from tevatron.arguments import ModelArguments, DataArguments, \\\n",
    "    TevatronTrainingArguments as TrainingArguments\n",
    "from trainer import TevatronTrainer\n",
    "from data import HFQueryDataset, HFCorpusDataset, HFTrainDataset, TrainDataset, TrainCollator\n",
    "\n",
    "from repllama import RepLLaMA\n",
    "from data import EncodeDataset, EncodeCollator\n",
    "from utils import replace_with_xformers_attention\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Load parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "    #model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file='./train_params.json')\n",
    "else:\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    model_args: ModelArguments\n",
    "    data_args: DataArguments\n",
    "    training_args: TrainingArguments\n",
    "\n",
    "if training_args.local_rank > 0 or training_args.n_gpu > 1:\n",
    "    raise NotImplementedError('Multi-GPU encoding is not supported.')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir, token='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'\n",
    "    )\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'train_dir': None,\n",
       "  'dataset_name': 'Tevatron/msmarco-passage',\n",
       "  'passage_field_separator': ' ',\n",
       "  'dataset_proc_num': 32,\n",
       "  'train_n_passages': 16,\n",
       "  'positive_passage_no_shuffle': False,\n",
       "  'negative_passage_no_shuffle': False,\n",
       "  'encode_in_path': None,\n",
       "  'encoded_save_path': None,\n",
       "  'encode_is_qry': False,\n",
       "  'encode_num_shard': 1,\n",
       "  'encode_shard_index': 0,\n",
       "  'q_max_len': 32,\n",
       "  'p_max_len': 196,\n",
       "  'data_cache_dir': None},\n",
       " None,\n",
       " 'train')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataclasses.asdict(data_args), model_args.cache_dir, data_args.dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = HFTrainDataset(tokenizer=tokenizer, data_args=data_args,\n",
    "                                   cache_dir=data_args.data_cache_dir or model_args.cache_dir)\n",
    "train_dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_id': '503854',\n",
       " 'query': 'stock option definition',\n",
       " 'positive_passages': [{'docid': '5912909',\n",
       "   'title': 'Definitions &Translations',\n",
       "   'text': 'stock option(noun) the right to buy or sell a stock at a specified price within a stated period. stock option(noun) a benefit given by a company to an employee in the form of an option to buy stock in the company at a discount or at a fixed price. stock options are not much use as an incentive if the price at which they can be exercised is out of reach.'}],\n",
       " 'negative_passages': [{'docid': '2718054',\n",
       "   'title': '-',\n",
       "   'text': 'Capital Management Outline Definition of bank capital Role of bank capital Capital adequacy Shareholdersâ\\x80\\x99 viewpoint Trends in bank capital Definition of bank capital Equity Common stock, preferred stock, surplus, and undivided profits equals the book value of equity.'},\n",
       "  {'docid': '1885414',\n",
       "   'title': 'Blocks',\n",
       "   'text': 'Brick shaped blocks provide stabilizing support, while oval shaped blocks are an ergonomic option for restorative and Iyengar yoga. You can pick from a variety of colors. You can stock up on yoga blocks and supplies as equipment for your studio.'},\n",
       "  {'docid': '6910146',\n",
       "   'title': '-',\n",
       "   'text': \"RE: Can you freeze crescent rolls/cinnamon rolls. Cooking them and freezing them is a good idea, I think I will try that option. My Schnucks has them on sale free after the coupon this week so I'm going to stock up!\"},\n",
       "  {'docid': '8645518',\n",
       "   'title': 'Taxation of Employee Stock Options',\n",
       "   'text': 'However it is, of course, subject to tax, and it is a preference item for the AMT (alternative minimum tax) calculation. When you exercise an incentive stock option there are a few different tax possibilities: 1  You exercise the incentive stock options, and sell the stock within the same calendar year. 2  You exercise the incentive stock options but hold the stock.'},\n",
       "  {'docid': '3872389',\n",
       "   'title': 'Stocks Basics: What Are Stocks?',\n",
       "   'text': 'The Definition of a Stock / Stock Certificate. A certificate establishing ownership of a stated number of shares in a corporations stock. Plain and simple, stock is a share in the ownership of a company. Stock represents a claim on the companys assets and earnings. As you acquire more stock, your ownership stake in the company becomes greater.'},\n",
       "  {'docid': '7667973',\n",
       "   'title': '-',\n",
       "   'text': 'The valuation of a stock appreciation right operates exactly like a stock option in that the employee benefits from any increases in stock price above the price set in the award.'},\n",
       "  {'docid': '8464680',\n",
       "   'title': 'How are stock warrants different from stock options?',\n",
       "   'text': 'A stock option is a contract between two people that gives the holder the right, but not the obligation, to buy or sell outstanding stocks at a specific price and at a specific date. Options are purchased when it is believed that the price of a stock will go up or down (depending on the option type).'},\n",
       "  {'docid': '1756722',\n",
       "   'title': 'Wealth management',\n",
       "   'text': 'Private wealth management[edit] Private wealth management is delivered to high-net-worth investors. Generally this includes advice on the use of various estate planning vehicles, business-succession or stock-option planning, and the occasional use of hedging derivatives for large blocks of stock.'},\n",
       "  {'docid': '6046262',\n",
       "   'title': '-',\n",
       "   'text': 'Definition: A publicly held company is a stock corporation in which the shares of stock are available to the public.'},\n",
       "  {'docid': '2263872',\n",
       "   'title': 'market value ratios',\n",
       "   'text': \"Definition. An equation that compares the current stock price to a financial indicator on the company's financial statements. The most often used indicator is a company's earnings per share. For example, if a stock price is $10 and its earnings are $2 per share, then the market value ratio would be 5-to-1. Market value ratios are typically used to determine if a company is over or undervalued.\"},\n",
       "  {'docid': '8537065',\n",
       "   'title': '-',\n",
       "   'text': \"If your employer grants you a statutory stock option, you generally don't include any amount in your gross income when you receive or exercise the option. However, you may be subject to alternative minimum tax in the year you exercise an ISO. For more information, refer to the Form 6251 Instructions.\"},\n",
       "  {'docid': '3193381',\n",
       "   'title': 'Stock Features - What is callable preferred stock Why do...',\n",
       "   'text': 'What is callable preferred stock ? Why do corporations issue such stock? Given the different features that are associated with stock (callable, cumulative, preferred, etc.), what type of stock would you want to buy personally and why? A callable preferred stock is when a company issues stock to its investor(s) at a fixed rate, but has the option of regaining the stock after a designated date in the future at the issued price. The stock can be purchased back at a certain price or at a higher or lower rate depending on the market value of the stock. Corporations issue such stock due to the cash producing potential from quarterly dividends and preferred stockholders cannot vote for any company issues. The different features that are associated with stock are callable, preferred, cumulative, and convertible stocks.'},\n",
       "  {'docid': '3206841',\n",
       "   'title': '-',\n",
       "   'text': 'The questions on the exam will cover the reading material, and will be very similar. to those in the practice sets. Question 1: Suppose an American call option is written on Nortel stock. The exercise price is $105 (â\\x98º) and. the present value of the exercise price is $100.'},\n",
       "  {'docid': '3226956',\n",
       "   'title': 'Taxation of Stock Warrants',\n",
       "   'text': 'A stock warrant is similar to a stock option in that both give you the right to purchase shares of the stock at a guaranteed strike price and you are able to exercise this right for a limited time. However, warrants are issued by a company for its own stock and are usually good for several years.hen you exercise warrants to buy the underlying stock, you pay the stated strike price to the issuing company. The difference between the strike price and the price of a share, minus the cost basis, is taxable income.'},\n",
       "  {'docid': '1965762',\n",
       "   'title': 'Stock Option Path to Riches - Part 1',\n",
       "   'text': 'An employee receiving a nonstatutory stock option may be taxed, in most cases, at any of the following times: (1) when the option is received; (2) when he exercises the option; or (3) when restrictions (if any) on disposition of the stock (acquired by the option) lapse.'},\n",
       "  {'docid': '917668',\n",
       "   'title': '-',\n",
       "   'text': '19, In The Money Covered Calls. In the money covered calls are those where an investor has sold a call option against stock he owns (hence, it is covered) where the strike price of the call option is less than the current stock price (so it is in the money).9, In The Money Covered Calls. In the money covered calls are those where an investor has sold a call option against stock he owns (hence, it is covered) where the strike price of the call option is less than the current stock price (so it is in the money).'},\n",
       "  {'docid': '4404143',\n",
       "   'title': '-',\n",
       "   'text': \"DEFINITION of 'Crossover' A crossover is the point on a stock chart when a security and an indicator intersect. Technical analysts use crossovers to aid in forecasting the future movements in the price of a stock. In most technical analysis models, a crossover is a signal to either buy or sell. The point on a stock chart when a security and an indicator intersect. Crossovers are used by technical analysts to aid in forecasting the future movements in the price of a stock. In most technical analysis models, a crossover is a signal to either buy or sell.\"},\n",
       "  {'docid': '8146496',\n",
       "   'title': 'option',\n",
       "   'text': \"to have the option of doing sth â\\x86\\x92 tener la posibilidad de hacer algo imprisonment without the option of bail (Jur) â\\x86\\x92 prisiÃ³n f preventiva to keep one's options open â\\x86\\x92 no descartar ninguna posibilidad. 2. (Comm) â\\x86\\x92 opciÃ³n f at the option of the purchaser â\\x86\\x92 a opciÃ³n del comprador stock option (Fin) â\\x86\\x92 compra f opcional de acciones to take out an option on another 100 â\\x86\\x92 suscribir una opciÃ³n para la compra de otros 100 with the option to buy â\\x86\\x92 con opciÃ³n de compra with an option on ten more aircraft â\\x86\\x92 con opciÃ³n para la compra de otros diez aviones. 3.\"},\n",
       "  {'docid': '4345264',\n",
       "   'title': 'The Difference Between Call and Put Options',\n",
       "   'text': 'Put Options. A put option is an agreement that gives the owner of that put the right, but not the obligation, to sell a set amount of an underlying stock or other asset at a specific price within a specific period of time. Therefore, the buyer of a put option has the right to sell their underlying shares at a set price.'},\n",
       "  {'docid': '971844',\n",
       "   'title': '-',\n",
       "   'text': \"What is a 'Grant'. A grant is the issuance of an award, such as a stock option, to key employees under a stock plan. A stock option grants the employee the right to purchase a certain number of shares of the company's stock at a predetermined price. This price is called the grant price.\"},\n",
       "  {'docid': '1124085',\n",
       "   'title': 'Stock exchange',\n",
       "   'text': 'The initial public offering of stocks and bonds to investors is by definition done in the primary market and subsequent trading is done in the secondary market. A stock exchange is often the most important component of a stock market.'},\n",
       "  {'docid': '8079813',\n",
       "   'title': 'MSCI Index and What It Measures',\n",
       "   'text': 'The MSCI tracks stock prices throughout the world. Definition: An MSCI Index is a measurement of stock market performance in a particular area. Like other indexes, such as the Dow Jones Averages or the S&P 500, it tracks the performance of the stocks included in the index. MSCI stands for Morgan Stanley Capital International'},\n",
       "  {'docid': '4042669',\n",
       "   'title': '-',\n",
       "   'text': \"However, a straddle would exist if the taxpayer owned one of the companies and also held an option on that company's stock that substantially diminished the taxpayer's risk of loss on that stock; see Sec.\"},\n",
       "  {'docid': '4083073',\n",
       "   'title': \"What is a 'Ticker Symbol'\",\n",
       "   'text': 'Ticker symbols for options are structured to represent the underlying stock ticker they are based on and also their expiration date and contract type (either a put or a call option). Mutual fund ticker symbols are usually alphanumeric and end with the letter X to differentiate them from stock symbols.icker symbols for options are structured to represent the underlying stock ticker they are based on and also their expiration date and contract type (either a put or a call option). Mutual fund ticker symbols are usually alphanumeric and end with the letter X to differentiate them from stock symbols.'},\n",
       "  {'docid': '7581591',\n",
       "   'title': 'What is an Option?',\n",
       "   'text': 'A call option is in-the-money if the current market value of the underlying stock is above the exercise price of the option. The call option is out-of-the-money if the stock is below the exercise price. A put option is in-the-money if the current market value of the underlying stock is below the exercise price. A put option is out-of-the-money if its underlying price is above the exercise price.'},\n",
       "  {'docid': '1309048',\n",
       "   'title': 'Stocks Basics: What Are Stocks?',\n",
       "   'text': \"Stocks Basics: What Are Stocks? Youâ\\x80\\x99ve probably heard a popular definition of what a stock is: â\\x80\\x9cA stock is a share in the ownership of a company. Stock represents a claim on the company's assets and earnings. As you acquire more stock, your ownership stake in the company becomes greater.â\\x80\\x9d Unfortunately, this definition is incorrect in some key ways. To start with, stock holders do not own corporations; they own shares issued by corporations.\"},\n",
       "  {'docid': '1014792',\n",
       "   'title': 'Delta',\n",
       "   'text': \"If the delta is 1, for example, the relationship of the prices is 1 to 1. That means there's a $1 change in the option price for every $1 change in the price of the underlying instrument. With a call option, an increase in the price of an underlying instrument typically results in an increase in the price of the option.elta. The change in the price of an option that results from a one-point change in the price of the underlying stock. For example, a delta of 0.5 indicates that the option will rise in price by 1 / 2 point (50Â¢) for each 1-point ($1) rise in the price of the underlying stock.\"},\n",
       "  {'docid': '869014',\n",
       "   'title': '-',\n",
       "   'text': \"DEFINITION of 'Grant'. The issuance of an award, such as a stock option, to key employees under a stock plan. A stock option grants the employee the right to purchase a certain number of shares of the company's stock at a predetermined price. There is usually a waiting period before an employee can exercise their stock options.\"},\n",
       "  {'docid': '5332522',\n",
       "   'title': 'Stock Price Definition',\n",
       "   'text': 'Stock Price Definition | What is it? (1) The market value of anything being offered for sale. (2) Market value of a share of common stock on the date shown. Highs and lows are based on the highest and lowest intra-day trading price.'},\n",
       "  {'docid': '804677',\n",
       "   'title': 'straddle',\n",
       "   'text': 'put option - the option to sell a given stock (or stock index or commodity future) at a given price before a given date put straddle, span - the act of sitting or standing astride'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.dataset[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_dataset.dataset[0:10000]['query']\n",
    "train_dataset.dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.dataset.size_in_bytes\n",
    "#dir(train_dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(data_args, train_dataset.process(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.total_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "model = RepLLaMA.build(\n",
    "        model_args,\n",
    "        training_args,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': 'repllama_test',\n",
       " 'overwrite_output_dir': True,\n",
       " 'do_train': False,\n",
       " 'do_eval': False,\n",
       " 'do_predict': False,\n",
       " 'evaluation_strategy': <IntervalStrategy.NO: 'no'>,\n",
       " 'prediction_loss_only': False,\n",
       " 'per_device_train_batch_size': 1,\n",
       " 'per_device_eval_batch_size': 32,\n",
       " 'per_gpu_train_batch_size': None,\n",
       " 'per_gpu_eval_batch_size': None,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'eval_accumulation_steps': None,\n",
       " 'eval_delay': 0,\n",
       " 'learning_rate': 0.0001,\n",
       " 'weight_decay': 0.0,\n",
       " 'adam_beta1': 0.9,\n",
       " 'adam_beta2': 0.999,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'num_train_epochs': 1,\n",
       " 'max_steps': 10000,\n",
       " 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,\n",
       " 'lr_scheduler_kwargs': {},\n",
       " 'warmup_ratio': 0.1,\n",
       " 'warmup_steps': 10,\n",
       " 'log_level': 'passive',\n",
       " 'log_level_replica': 'warning',\n",
       " 'log_on_each_node': True,\n",
       " 'logging_dir': 'repllama_test/runs/Mar15_15-57-29_alex-gpu-1',\n",
       " 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'logging_first_step': False,\n",
       " 'logging_steps': 10,\n",
       " 'logging_nan_inf_filter': True,\n",
       " 'save_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'save_steps': 200,\n",
       " 'save_total_limit': None,\n",
       " 'save_safetensors': True,\n",
       " 'save_on_each_node': False,\n",
       " 'save_only_model': False,\n",
       " 'no_cuda': False,\n",
       " 'use_cpu': False,\n",
       " 'use_mps_device': False,\n",
       " 'seed': 42,\n",
       " 'data_seed': None,\n",
       " 'jit_mode_eval': False,\n",
       " 'use_ipex': False,\n",
       " 'bf16': True,\n",
       " 'fp16': False,\n",
       " 'fp16_opt_level': 'O1',\n",
       " 'half_precision_backend': 'auto',\n",
       " 'bf16_full_eval': False,\n",
       " 'fp16_full_eval': False,\n",
       " 'tf32': None,\n",
       " 'local_rank': 0,\n",
       " 'ddp_backend': None,\n",
       " 'tpu_num_cores': None,\n",
       " 'tpu_metrics_debug': False,\n",
       " 'debug': [],\n",
       " 'dataloader_drop_last': False,\n",
       " 'eval_steps': None,\n",
       " 'dataloader_num_workers': 0,\n",
       " 'past_index': -1,\n",
       " 'run_name': 'repllama_test',\n",
       " 'disable_tqdm': False,\n",
       " 'remove_unused_columns': True,\n",
       " 'label_names': None,\n",
       " 'load_best_model_at_end': False,\n",
       " 'metric_for_best_model': None,\n",
       " 'greater_is_better': None,\n",
       " 'ignore_data_skip': False,\n",
       " 'fsdp': [],\n",
       " 'fsdp_min_num_params': 0,\n",
       " 'fsdp_config': {'min_num_params': 0,\n",
       "  'xla': False,\n",
       "  'xla_fsdp_grad_ckpt': False},\n",
       " 'fsdp_transformer_layer_cls_to_wrap': None,\n",
       " 'deepspeed': None,\n",
       " 'label_smoothing_factor': 0.0,\n",
       " 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>,\n",
       " 'optim_args': None,\n",
       " 'adafactor': False,\n",
       " 'group_by_length': False,\n",
       " 'length_column_name': 'length',\n",
       " 'report_to': [],\n",
       " 'ddp_find_unused_parameters': None,\n",
       " 'ddp_bucket_cap_mb': None,\n",
       " 'ddp_broadcast_buffers': None,\n",
       " 'dataloader_pin_memory': True,\n",
       " 'dataloader_persistent_workers': False,\n",
       " 'skip_memory_metrics': True,\n",
       " 'use_legacy_prediction_loop': False,\n",
       " 'push_to_hub': False,\n",
       " 'resume_from_checkpoint': None,\n",
       " 'hub_model_id': None,\n",
       " 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,\n",
       " 'hub_token': None,\n",
       " 'hub_private_repo': False,\n",
       " 'hub_always_push': False,\n",
       " 'gradient_checkpointing': False,\n",
       " 'gradient_checkpointing_kwargs': None,\n",
       " 'include_inputs_for_metrics': False,\n",
       " 'fp16_backend': 'auto',\n",
       " 'push_to_hub_model_id': None,\n",
       " 'push_to_hub_organization': None,\n",
       " 'push_to_hub_token': None,\n",
       " '_n_gpu': 1,\n",
       " 'mp_parameters': '',\n",
       " 'auto_find_batch_size': False,\n",
       " 'full_determinism': False,\n",
       " 'torchdynamo': None,\n",
       " 'ray_scope': 'last',\n",
       " 'ddp_timeout': 1800,\n",
       " 'torch_compile': False,\n",
       " 'torch_compile_backend': None,\n",
       " 'torch_compile_mode': None,\n",
       " 'dispatch_batches': None,\n",
       " 'split_batches': False,\n",
       " 'include_tokens_per_second': False,\n",
       " 'include_num_input_tokens_seen': False,\n",
       " 'neftune_noise_alpha': None,\n",
       " 'negatives_x_device': True,\n",
       " 'do_encode': False,\n",
       " 'grad_cache': False,\n",
       " 'gc_q_chunk_size': 4,\n",
       " 'gc_p_chunk_size': 32}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_steps = int(np.ceil(train_dataset.total_len / training_args.per_device_train_batch_size))\n",
    "training_args.max_steps = max_steps\n",
    "\n",
    "dataclasses.asdict(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer_cls = TevatronTrainer\n",
    "trainer = trainer_cls(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=TrainCollator(\n",
    "            tokenizer,\n",
    "            max_p_len=data_args.p_max_len,\n",
    "            max_q_len=data_args.q_max_len\n",
    "        ),\n",
    "    )\n",
    "train_dataset.trainer = trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='201' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  201/10000 03:05 < 2:32:16, 1.07 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.764100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.349200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.468900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.307600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.671800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.522200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.951600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.764200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.246900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.981400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2024 16:00:43 - INFO - trainer -   Saving model checkpoint to repllama_test/tmp-checkpoint-200\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/home/azureuser/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/huggingface_hub-0.21.4-py3.8.egg/huggingface_hub/templates/modelcard_template.md'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1541\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1543\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m     )\n",
      "File \u001b[0;32m~/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/transformers/trainer.py:2302\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2299\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[1;32m   2303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/transformers/trainer.py:2378\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2377\u001b[0m     staging_output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(staging_output_dir, _internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   2381\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   2382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(staging_output_dir)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/transformers/trainer.py:2886\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   2883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   2885\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2886\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save(output_dir)\n\u001b[1;32m   2888\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/data/code/tevatron/examples/repllama/trainer.py:24\u001b[0m, in \u001b[0;36mTevatronTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m     22\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving model checkpoint to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, output_dir)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(output_dir)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_deepspeed_zero3_enabled():\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/data/code/tevatron/examples/repllama/repllama.py:121\u001b[0m, in \u001b[0;36mRepLLaMA.save\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_dir: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_q\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/peft/peft_model.py:210\u001b[0m, in \u001b[0;36mPeftModel.save_pretrained\u001b[0;34m(self, save_directory, safe_serialization, selected_adapters, save_embedding_layers, is_main_process, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_main_process:\n\u001b[1;32m    209\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(save_directory, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_or_update_model_card(save_directory)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m adapter_name \u001b[38;5;129;01min\u001b[39;00m selected_adapters:\n\u001b[1;32m    213\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\n",
      "File \u001b[0;32m~/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/peft/peft_model.py:788\u001b[0m, in \u001b[0;36mPeftModel.create_or_update_model_card\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;124;03mUpdates or create model card to include information about peft:\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;124;03m1. Adds `peft` library tag\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03m4. Adds quantization information if it was used\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    786\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREADME.md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 788\u001b[0m card \u001b[38;5;241m=\u001b[39m ModelCard\u001b[38;5;241m.\u001b[39mload(filename) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filename) \u001b[38;5;28;01melse\u001b[39;00m ModelCard\u001b[38;5;241m.\u001b[39mfrom_template(ModelCardData())\n\u001b[1;32m    790\u001b[0m card\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrary_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    792\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/huggingface_hub-0.21.4-py3.8.egg/huggingface_hub/repocard.py:407\u001b[0m, in \u001b[0;36mModelCard.from_template\u001b[0;34m(cls, card_data, template_path, **template_kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_template\u001b[39m(  \u001b[38;5;66;03m# type: ignore # violates Liskov property but easier to use\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtemplate_kwargs,\n\u001b[1;32m    341\u001b[0m ):\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize a ModelCard from a template. By default, it uses the default template, which can be found here:\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfrom_template(card_data, template_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtemplate_kwargs)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/huggingface_hub-0.21.4-py3.8.egg/huggingface_hub/repocard.py:325\u001b[0m, in \u001b[0;36mRepoCard.from_template\u001b[0;34m(cls, card_data, template_path, **template_kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m card_data\u001b[38;5;241m.\u001b[39mto_dict()\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    324\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(template_kwargs)  \u001b[38;5;66;03m# Template_kwargs have priority\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m template \u001b[38;5;241m=\u001b[39m jinja2\u001b[38;5;241m.\u001b[39mTemplate(Path(template_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_template_path)\u001b[38;5;241m.\u001b[39mread_text())\n\u001b[1;32m    326\u001b[0m content \u001b[38;5;241m=\u001b[39m template\u001b[38;5;241m.\u001b[39mrender(card_data\u001b[38;5;241m=\u001b[39mcard_data\u001b[38;5;241m.\u001b[39mto_yaml(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(content)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/train_emb3/lib/python3.11/pathlib.py:1058\u001b[0m, in \u001b[0;36mPath.read_text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1058\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/data/miniconda3/envs/train_emb3/lib/python3.11/pathlib.py:1044\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1043\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m, mode, buffering, encoding, errors, newline)\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/home/azureuser/data/miniconda3/envs/train_emb3/lib/python3.11/site-packages/huggingface_hub-0.21.4-py3.8.egg/huggingface_hub/templates/modelcard_template.md'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepLLaMA(\n",
       "  (lm_q): PeftModelForFeatureExtraction(\n",
       "    (base_model): LoraModel(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_p): PeftModelForFeatureExtraction(\n",
       "    (base_model): LoraModel(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cross_entropy): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
